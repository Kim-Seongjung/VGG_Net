{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2221, 64, 64, 3)\n",
      "64 64\n",
      "64 64\n"
     ]
    }
   ],
   "source": [
    "#conv Neural Network\n",
    "# tensorboard --logdir=/home/ncc/notebook/learn/tensorboard/log\n",
    "\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os \n",
    "\n",
    "file_locate='/media/seongjung/Seagate Backup Plus Drive/data/test/'\n",
    "sess = tf.InteractiveSession()\n",
    "test_img=np.load(file_locate+'test_img.npy');\n",
    "print np.shape(test_img)\n",
    "img_row = np.shape(test_img)[1]\n",
    "img_col = np.shape(test_img)[2]\n",
    "\n",
    "batch_size=30\n",
    "print img_row ,img_col\n",
    "n_classes =2\n",
    "\n",
    "\n",
    "\n",
    "in_ch =3\n",
    "out_ch1=30\n",
    "out_ch2=30\n",
    "out_ch3=30\n",
    "out_ch4=30\n",
    "out_ch5=30\n",
    "out_ch6=30\n",
    "out_ch7=30\n",
    "out_ch8=30\n",
    "out_ch9=30\n",
    "out_ch10=30\n",
    "out_ch11=30\n",
    "out_ch12=30\n",
    "out_ch13=30\n",
    "\n",
    "fully_ch1=1024\n",
    "fully_ch2 =1024\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "out_ch1=512\n",
    "out_ch2=512\n",
    "out_ch3=512\n",
    "out_ch4=512\n",
    "out_ch5=512\n",
    "out_ch6=512\n",
    "out_ch7=512\n",
    "out_ch8=512\n",
    "out_ch9=512\n",
    "out_ch10=512\n",
    "out_ch11=512\n",
    "out_ch12=512\n",
    "out_ch13=512\n",
    "fully_ch1=4096\n",
    "fully_ch2 =4096\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x= tf.placeholder(\"float32\",shape=[None,img_col , img_row , 3],  name = 'x-input')\n",
    "y_=tf.placeholder(\"float32\",shape=[None , n_classes] , name = 'y-input')\n",
    "keep_prob = tf.placeholder(\"float32\")\n",
    "\n",
    "x_image= tf.reshape(x,[-1,img_row,img_col,3])\n",
    "\n",
    "iterate=300\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "weight_row =3 ; weight_col=3\n",
    "\n",
    "pooling_row_size1=int(img_row/2)\n",
    "pooling_row_size2=int(pooling_row_size1/2)\n",
    "pooling_row_size3=int(pooling_row_size2/2)\n",
    "pooling_row_size4=int(pooling_row_size3/2)\n",
    "pooling_row_size5=int(pooling_row_size4/2)\n",
    "pooling_col_size1=int(img_col/2)\n",
    "pooling_col_size2=int(pooling_col_size1/2)\n",
    "pooling_col_size3=int(pooling_col_size2/2)\n",
    "pooling_col_size4=int(pooling_col_size3/2)\n",
    "pooling_col_size5=int(pooling_col_size4/2)\n",
    "\n",
    "print img_col , img_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data (2221, 64, 64, 3)\n",
      "Training Data Label (2221, 2)\n",
      "Test Data Label (2221, 2)\n",
      "val Data Label (2221, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    #with tf.device('/gpu:1'):\n",
    "    train_img=np.load(file_locate+'train_img.npy');\n",
    "    train_lab=np.load(file_locate+'train_lab.npy');\n",
    "    val_img= np.load(file_locate+'val_img.npy');\n",
    "    val_lab = np.load(file_locate+'val_lab.npy');\n",
    "    test_img=np.load(file_locate+'test_img.npy');\n",
    "    test_lab=np.load(file_locate+'test_lab.npy');\n",
    "\n",
    "    print \"Training Data\",np.shape(train_img)\n",
    "    print \"Training Data Label\",np.shape(train_lab)\n",
    "    print \"Test Data Label\",np.shape(test_lab)\n",
    "    print \"val Data Label\" , np.shape(val_img)\n",
    "\n",
    "    n_train= np.shape(train_img)[0]\n",
    "    n_train_lab = np.shape(train_lab)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'val_img=val_img[:100]\\nval_lab=val_lab[:100]\\ntest_img=test_img[:100]\\ntest_lab=test_lab[:100]\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"val_img=val_img[:100]\n",
    "val_lab=val_lab[:100]\n",
    "test_img=test_img[:100]\n",
    "test_lab=test_lab[:100]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    def next_batch(batch_size , image , label):\n",
    "\n",
    "        a=np.random.randint(np.shape(image)[0] -batch_size)\n",
    "        batch_x = image[a:a+batch_size,:]\n",
    "        batch_y= label[a:a+batch_size,:]\n",
    "        return batch_x, batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, [1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "a=(3,[1,2,3])\n",
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_kernel(layer_name ,weights_info, biases_info,device_name, init = 'xavier' ):\n",
    "    \"\"\"\n",
    "    weight_info= (weight_name ,[weight_row, weight_col , n_in_ch , n_out_ch])\n",
    "    biase_info = ()\n",
    "    return ret_weighs , ret_biases\n",
    "    \"\"\"\n",
    "    #check weight_name's implement is 'tensor'\n",
    "    #check wieght_name's implement is 'tensor'\n",
    "\n",
    "    with tf.device(device_name):\n",
    "        weights_name=weights_info[0]\n",
    "        weights_shape=weights_info[1]\n",
    "        biases_name = biases_info[0]\n",
    "        biases_shape = biases_info[1]\n",
    "        # make weights variables \n",
    "        with tf.variable_scope(layer_name) as scope:\n",
    "            try: #처음 변수를 만드는 거면 그냥 만든다,\n",
    "                if init == 'xavier':\n",
    "                    ret_weights=tf.get_variable( weights_name , weights_shape , initializer = tf.contrib.layers.xavier_initializer())\n",
    "                    print 'a'\n",
    "            except:\n",
    "                print \" this weight and biases already existed! reuse those variable\"\n",
    "                scope.reuse_variables()\n",
    "                if init=='xavier':\n",
    "                    print 'aa'\n",
    "                    ret_weights=tf.get_variable( weights_name , weights_shape , initializer = tf.contrib.layers.xavier_initializer())\n",
    "                    \n",
    "                    \n",
    "        # make biases variables             \n",
    "        with tf.variable_scope(layer_name) as scope:\n",
    "            try: \n",
    "                initial= tf.constant(0.1 , shape= biases_shape) \n",
    "                ret_biases= tf.Variable(initial , name = biases_name)  \n",
    "            except:\n",
    "                scope.reuse_variables()                \n",
    "                initial= tf.constant(0.1 ,  shape =biases_shape) \n",
    "                ret_biases= tf.Variable(initial , name = biases_name) \n",
    "        \n",
    "        return ret_weights , ret_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pooling(x,stype , kernel_size , strides , padding='SAME'):\n",
    "    \n",
    "    \"\"\"\n",
    "     pooling ='max'\n",
    "    \"\"\"\n",
    "    if stype == 'max':\n",
    "        ret_x = tf.nn.avg_pool(x , kernel_size , strides , padding)\n",
    "    elif stype == 'avg ':\n",
    "        ret_x = tf.nn.max_pool(x, kernel_size , strides , padding)\n",
    "    return ret_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_fully_layer(x, weights ):\n",
    "    #def make_kernel(layer_name ,weights_info, biases_info,device_name, init = 'xavier' ):\n",
    "    print 'make_fully_layer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model_save(condition ):\n",
    "    print 'model_save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def model_restore(condition):\n",
    "    print 'model_restore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of conv layer :  13\n"
     ]
    }
   ],
   "source": [
    "\n",
    "L_conv_weights_shape=[[3,3,in_ch,out_ch1],[3,3,out_ch1,out_ch2],[3,3,out_ch2,out_ch3],[3,3,out_ch3,out_ch4],[3,3,out_ch4,out_ch5],\\\n",
    "                      [3,3,out_ch5,out_ch6],[3,3,out_ch6,out_ch7],[3,3,out_ch7,out_ch8],[3,3,out_ch8,out_ch9],\\\n",
    "                      [3,3,out_ch9,out_ch10],[3,3,out_ch10,out_ch11],[3,3,out_ch11,out_ch12],[3,3,out_ch12,out_ch13]]\n",
    "L_conv_biases_shape =[[out_ch1],[out_ch2],[out_ch3],[out_ch4],[out_ch5],[out_ch6],[out_ch7],[out_ch8],\\\n",
    "                   [out_ch9],[out_ch10],[out_ch11],[out_ch12],[out_ch13]]\n",
    "L_conv_strides_shape =[[1,1,1,1],[1,1,1,1],[1,1,1,1],[1,1,1,1],[1,1,1,1],[1,1,1,1],[1,1,1,1],[1,1,1,1],\\\n",
    "                   [1,1,1,1],[1,1,1,1],[1,1,1,1],[1,1,1,1],[1,1,1,1]]\n",
    "L_fc_ch=[1024, 1024,1024]\n",
    "n_fc_ch=len(L_fc_ch)\n",
    "n_conv_layer = len(L_conv_weights_shape)\n",
    "print 'number of conv layer : ', n_conv_layer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confirm_usable_device():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU' or x.device_type == 'CPU']    \n",
    "# use this lib in tensorflow 'from tensorflow.python.client import device_lib'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def flat_img(conv ,row , col , n_ch ):\n",
    "    conv=tf.reshape( conv , [-1 , ,row * col * n_ch])\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'/cpu:0', u'/gpu:0']\n",
      "[u'/gpu:0']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "usable_device=confirm_usable_device()\n",
    "\n",
    "print usable_device\n",
    "usable_gpu=usable_device[1:]\n",
    "print usable_gpu\n",
    "\n",
    "n_usable_gpu = len(usable_gpu)\n",
    "usable_device_cpu=usable_device[0]\n",
    "print n_usable_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    if i==3:\n",
    "        print i\n",
    "print L_conv_strides_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def a(a):\n",
    "    print a\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "b=[1,2,3]\n",
    "a(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_conv_layer(x  , weights ,biases ,strides_ , device , activation_func='relu'):\n",
    "    \"\"\"\n",
    "    \n",
    "    params: \n",
    "    고민할 것들: 칼라channel을 동시에 convolution 하는 것을 고려해야 한다 그러면 strides 가 1,x,y,3 이 되야 하는데 \n",
    "    과연 이게 옳을까? 고민이다.\n",
    "    strides format = e.g) [1,1,1,1,]\n",
    "    pooling--> 'avg' , 'max' , 'None'  \n",
    "    activation_func = 'relu' , 'None' , 'sigmoid'\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool5_flat , w_fc1)+ b_fc1)\n",
    "        tf.nn.avg_pool(value, ksize, strides, padding, data_format='NHWC', name=None)\n",
    "        return tf.nn.max_pool(x , ksize=[1,2,2,1] ,strides = [1,2,2,1] , padding = 'SAME')\n",
    "        \n",
    "    \"\"\"\n",
    "    with tf.device(device):\n",
    "        print type(weights)\n",
    "        conv=tf.nn.conv2d(x, weights , strides=[1,1,1,1] , padding='SAME') + biases\n",
    "        #set activation function \n",
    "        if activation_func== 'relu':\n",
    "            ret_x=tf.nn.relu(conv)\n",
    "        elif activation_func == 'sigmoid':\n",
    "            ret_x=tf.sigmoid(conv)\n",
    "        #set pooling style\n",
    "        \n",
    "        return ret_x\n",
    "    \n",
    "    #ValueError: Dimensions must be equal, but are 1 and 3 for 'Conv2D_22' (op: 'Conv2D') with input shapes: [?,64,64,1], [1,3,3,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-25-52ea177f3130>, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-52ea177f3130>\"\u001b[0;36m, line \u001b[0;32m38\u001b[0m\n\u001b[0;31m    =L_ret_conv[-1].get_shape()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "L_ret_conv=[]\n",
    "L_weights=[]\n",
    "L_biases=[]\n",
    "L_fc_weights=[]\n",
    "L_fc_biases=[]\n",
    "#make conv layer\n",
    "for i in range(n_conv_layer+n_fc_ch): #mk layer \n",
    "    print i\n",
    "    layer_name= 'layer'+str(i)\n",
    "    weights_name='W'+str(i)\n",
    "    weights_info=(weights_name , L_conv_weights_shape[i])\n",
    "    biases_name = 'B'+str(i)\n",
    "    biases_info = (biases_name , L_conv_biases_shape[i])\n",
    "    \n",
    "    device_name = usable_gpu[i%n_usable_gpu]\n",
    "    weights , biases= make_kernel(layer_name , weights_info , biases_info, device_name)\n",
    "    L_weights.append(weights)\n",
    "    L_biases.append(biases)\n",
    "    \n",
    "    \n",
    "    if i==0:\n",
    "        tf.nn.conv2d(x, L_weights[i] ,strides=[1,1,1,1], padding='SAME')\n",
    "        conv=make_conv_layer(x , L_weights[i] , L_biases[i] , L_conv_strides_shape[i], device_name , activation_func ='relu')\n",
    "        print conv.get_shape()\n",
    "        L_ret_conv.append(conv)       \n",
    "    else:\n",
    "        print conv.get_shape()\n",
    "        print L_weights[i].get_shape()\n",
    "        conv=make_conv_layer(conv ,L_weights[i] , L_biases[i] ,L_conv_strides_shape[i], device_name , activation_func ='relu' )\n",
    "        print conv.get_shape()\n",
    "        L_ret_conv.append(conv)\n",
    "    if i ==n_conv_layer: # convolution 은 끝났고 이제 fully connected layer을 만든는 것이다. flat된 conv_img 에서도 필요하다.\n",
    "         #flat layer \n",
    "            last_conv_row , last_conv_col , last_conv_n_ch=L_ret_conv[-1].get_shape()[1:]\n",
    "            flatted_conv=flat_img(L_ret_conv[-1] , last_conv_row , last_conv_col , last_conv_ch)\n",
    "            fc_weights, fc_biases =make_kernel('fc1' ,fc_weights_info , fc_biases_info, device_name)\n",
    "            L_fc_weights.append(fc_weights)\n",
    "            L_fc_biases.append(fc_biases)\n",
    "            \n",
    "            \n",
    "            \n",
    "            fc_weights_info=[] \n",
    "            make_kernel('fc1' , )\n",
    "            flat_layer(L_ret_conv[-1])\n",
    "            =L_ret_conv[-1].get_shape()\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    #def make_conv_layer(x  , weights ,biases,kernel_size ,strides , device , activation_func):\n",
    "#make fully connected layer \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "a=[1,2,3]\n",
    "print a[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pooling_col_size4=int(h_pool13.get_shape()[2])\n",
    "pooling_row_size4=int(h_pool13.get_shape()[1])\n",
    "#connect fully connected layer \n",
    "with tf.device('/gpu:2'):\n",
    "    w_fc1=tf.get_variable(\"fc1\",[pooling_col_size4*pooling_row_size4*out_ch13,fully_ch1] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b_fc1 = bias_variable([fully_ch1])\n",
    "\n",
    "    h_pool5_flat =tf.reshape(h_pool13, [-1,pooling_col_size4*pooling_row_size4*out_ch13])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool5_flat , w_fc1)+ b_fc1)\n",
    "with tf.device('/gpu:3'):\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    w_fc2 =tf.get_variable(\"fc2\",[fully_ch1 , n_classes],initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b_fc2 = bias_variable([n_classes])\n",
    "\n",
    "    y_conv=tf.add(tf.matmul(h_fc1_drop,w_fc2),b_fc2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dirname = '/home/ncc/notebook/mammo/result/'\n",
    "\n",
    "dirname='/mnt/Jupyter/seongjung_mnt/Eye/result/'\n",
    "\n",
    "count=0\n",
    "while(True):\n",
    "    if not os.path.isdir(dirname):\n",
    "        os.mkdir(dirname)\n",
    "        break\n",
    "    elif not os.path.isdir(dirname + str(count)):\n",
    "        dirname=dirname+str(count)\n",
    "        os.mkdir(dirname)\n",
    "        break\n",
    "    else:\n",
    "        count+=1\n",
    "print 'it is recorded at :'+str(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f=open(dirname+\"/log.txt\",'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:2'):\n",
    "#sm_conv= tf.nn.softmax(y_conv)\n",
    "    #cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "    start_time = time.time()\n",
    "\n",
    "    regular=0.01*(tf.reduce_sum(tf.square(y_conv)))\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits( y_conv, y_))\n",
    "with tf.device('/gpu:3'):\n",
    "    cost = cost+regular\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cost) #1e-4\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        with tf.name_scope('correct_prediction'):\n",
    "            correct_prediction = tf.equal(tf.argmax(y_conv,1) ,tf.argmax(y_,1))\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction , \"float\")) \n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "for i in range(iterate):\n",
    "    \n",
    "    batch_xs , batch_ys = next_batch(batch_size, train_img , train_lab)\n",
    "   # batch_val_xs  , batch_val_ys = next_batch(20 , val_img , val_lab)\n",
    "    if i%100 ==0: # in here add to validation \n",
    "        try:\n",
    "            train_accuracy = sess.run( accuracy , feed_dict={x:val_img , y_:val_lab , keep_prob: 1.0})        \n",
    "            loss = sess.run(cost , feed_dict = {x:val_img , y_: val_lab , keep_prob: 1.0})\n",
    "\n",
    "            #result = sess.run(sm_conv , feed_dict = {x:val_img , y_:batch_ys , keep_prob :1.0})\n",
    "            print(\"step %d , training  accuracy %g\" %(i,train_accuracy))\n",
    "            print(\"step %d , loss : %g\" %(i,loss))\n",
    "            str_ = 'step :'+str(i)+'loss :'+str(loss) +'training accuracy :'+str(train_accuracy)+'\\n'\n",
    "            f.write(str_)\n",
    "    \n",
    "        except :\n",
    "            list_acc=[]\n",
    "            list_loss=[]\n",
    "            n_divide=len(val_img)/batch_size\n",
    "            for j in range(n_divide):\n",
    "                \n",
    "                # j*batch_size :(j+1)*batch_size\n",
    "                train_accuracy,loss = sess.run([accuracy ,cost], feed_dict={x:val_img[ j*batch_size :(j+1)*batch_size] , y_:val_lab[ j*batch_size :(j+1)*batch_size ] , keep_prob: 1.0})        \n",
    "                list_acc.append(float(train_accuracy))\n",
    "                list_loss.append(float(loss))\n",
    "            train_accuracy , loss=sess.run([accuracy,cost] , feed_dict={x:val_img[(j+1)*batch_size : ] , y_:val_lab[(j+1)*(batch_size) : ] , keep_prob : 1.0})\n",
    "            #right above code have to modify\n",
    "            \n",
    "            list_acc.append(train_accuracy)\n",
    "            list_loss.append(loss)\n",
    "            list_acc=np.asarray(list_acc)\n",
    "            list_loss= np.asarray(list_loss)\n",
    "            \n",
    "            train_accuracy=np.mean(list_acc)\n",
    "            loss = np.mean(list_loss)\n",
    "            \n",
    "            #result = sess.run(sm_conv , feed_dict = {x:val_img , y_:batch_ys , keep_prob :1.0})\n",
    "            print(\"step %d , training  accuracy %g\" %(i,train_accuracy))\n",
    "            print(\"step %d , loss : %g\" %(i,loss))\n",
    "            str_ = 'step :'+str(i)+'loss :'+str(loss) +'training accuracy :'+str(train_accuracy)+'\\n'\n",
    "            f.write(str_)\n",
    "    \n",
    "    \n",
    "    sess.run(train_step ,feed_dict={x:batch_xs , y_:batch_ys , keep_prob : 0.7})\n",
    "print(\"test accuracy %g\" %sess.run(accuracy , \n",
    "                                   feed_dict={x:test_img , y_:test_lab, keep_prob : 1.0 }))\n",
    "\n",
    "print(\"--- Training Time : %s ---\" % (time.time() - start_time))\n",
    "str_='test accuracy'+str(accuracy)\n",
    "f.write(str_)\n",
    "#추가 한 부분 \n",
    "test_pred , test_acc = sess.run([y_conv , accuracy] , feed_dict={x:test_img , y_:test_lab,keep_prob : 1.0})\n",
    "f.write(str(test_acc))\n",
    "\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def file2Graph(file_path , save_path , graph_size):\n",
    "    assert type(graph_size) == tuple \n",
    "    \n",
    "    step=[]\n",
    "    loss=[]\n",
    "    acc=[]\n",
    "    test_acc=0.0\n",
    "    fo=open(file_path)\n",
    "    lines=fo.readlines()\n",
    "    for line in lines:\n",
    "        list_splited_line=line.split()\n",
    "        if 'step' in list_splited_line:\n",
    "            print list_splited_line[2]\n",
    "            step.append(float(list_splited_line[2]))\n",
    "            \n",
    "        if 'loss' in line:\n",
    "            loss.append(float(list_splited_line[5]))\n",
    "        if 'training accuracy' in line:\n",
    "            acc.append(float(list_splited_line[9]))\n",
    "        if 'test accuracy' in line:\n",
    "            test_acc=float(list_splited_line[3])\n",
    "            \n",
    "    #need 4 4graph \n",
    "    fig1 = plt.figure(figsize =graph_size)\n",
    "    save_name='x: step , y : loss'\n",
    "    ax=fig1.add_subplot(1,1,1)\n",
    "    plt.plot(step , loss)\n",
    "    plt.savefig(save_path+save_name)\n",
    "    \n",
    "    fig2 = plt.figure(figsize =graph_size)\n",
    "    ax1=fig2.add_subplot(1,1,1)\n",
    "    save_name='x: step , y : acc'\n",
    "    plt.plot(step , acc)\n",
    "    plt.savefig(save_path+save_name)\n",
    "    \n",
    "    fig3 = plt.figure(figsize =graph_size)\n",
    "    ax2=fig3.add_subplot(1,1,1)\n",
    "    \n",
    "    save_name='x: step , y(blue) : acc , y2(green) : loss'\n",
    "    plt.plot(step , acc ,'b')\n",
    "    plt.plot(step , loss ,'g')\n",
    "    plt.savefig(save_path+save_name)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import Image\n",
    "result_list=list(result_np)\n",
    "lab_list=list(lab_np)\n",
    "pred_list=list(pred_np)\n",
    "#\n",
    "#def savepic(save_path,extension,img_source,lab_source, pred_list , result_list=None, img_row , img_col,color_ch=1 ):\n",
    "\n",
    "import Utility_\n",
    "save_path ='/home/ubuntu/Desktop/delete_'\n",
    "extension='.jpg'\n",
    "img_source = test_img\n",
    "print np.shape(img_source)\n",
    "if 'numpy'  in str(type(test_img)):\n",
    "    print 'numpy'\n",
    "print type(img_source)\n",
    "img=Utility_.savepic(save_path , extension , img_source , lab_list , pred_list , result_list, img_row=64 ,\\\n",
    "                 img_col=64 , color_ch =1)\n",
    "img=img.reshape(img_row , img_col)\n",
    "print np.shape(img)\n",
    "plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
